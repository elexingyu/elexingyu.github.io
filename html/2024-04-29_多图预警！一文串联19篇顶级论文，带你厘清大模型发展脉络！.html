<html>
<head>
<title></title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0,viewport-fit=cover">
<style>
*{margin:0;padding:0}html{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;line-height:1.6}img{z-index:999;position:relative;max-width:100%;margin:10px 0;}body{letter-spacing:.034em}h1,h2,h3,h4,h5,h6{font-weight:400;font-size:16px}a{color:#576b95;text-decoration:none;-webkit-tap-highlight-color:rgba(0,0,0,0)}td,th{word-wrap:break-word;padding:5px 10px;border:1px solid #DDD;}table{margin-bottom:10px;border-collapse:collapse;display:table;width:100%!important;}.appmsg_skin_default .rich_media_area_primary{background-color:#fff}.appmsg_skin_default .rich_media_area_primary .weui-loadmore_line .weui-loadmore__tips{background-color:#fff}.rich_media_area_primary{padding:20px 16px 12px;background-color:#fafafa}@media (max-width:375px){.rich_media_area_primary{padding:20px 60px 15px 60px}.rich_media_area_extra{padding:0 60px 21px 60px}}@media (min-width:1024px){.rich_media_area_primary_inner,.rich_media_area_extra_inner,body{max-width:677px;margin-left:auto;margin-right:auto}.rich_media_area_primary{padding-top:32px}}.rich_media{padding:20px;overflow:hidden;}.appmsg_skin_default .rich_media_area_primary{background-color:#fff}.appmsg_skin_default .rich_media_area_primary .weui-loadmore_line .weui-loadmore__tips{background-color:#fff}@media screen and (min-width:1024px){.rich_media_area_primary_inner,.rich_media_area_extra_inner{max-width:677px;margin-left:auto;margin-right:auto}.rich_media_area_primary{padding-top:32px}}.rich_media_content{overflow:hidden;color:#333;font-size:17px;word-wrap:break-word;-webkit-hyphens:auto;-ms-hyphens:auto;hyphens:auto;text-align:justify;position:relative;z-index:0}.rich_media_content *{max-width:100%!important;box-sizing:border-box!important;-webkit-box-sizing:border-box!important;word-wrap:break-word!important}.rich_media_content p{clear:both;min-height:1em}.rich_media_content em{font-style:italic}.rich_media_content fieldset{min-width:0}.rich_media_content .list-paddingleft-1,.rich_media_content .list-paddingleft-2,.rich_media_content .list-paddingleft-3{padding-left:2.2em}.rich_media_content .list-paddingleft-1 .list-paddingleft-2,.rich_media_content .list-paddingleft-2 .list-paddingleft-2,.rich_media_content .list-paddingleft-3 .list-paddingleft-2{padding-left:30px}.rich_media_content .list-paddingleft-1{padding-left:1.2em}.rich_media_content .list-paddingleft-3{padding-left:3.2em}.rich_media_content .code-snippet,.rich_media_content .code-snippet__fix{max-width:1000%!important}.rich_media_content .code-snippet *,.rich_media_content .code-snippet__fix *{max-width:1000%!important}.rich_media_title{font-size:22px;line-height:1.4;margin-bottom:13px;padding-bottom:13px;border-bottom:1px solid #e7e7eb;}@supports(-webkit-overflow-scrolling:touch){.rich_media_title{font-weight:700}}.rich_media_meta{display:inline-block;vertical-align:middle;padding:0 0 10px 0;font-size:15px;-webkit-tap-highlight-color:rgba(0,0,0,0)}.rich_media_meta.icon_appmsg_tag{margin-right:0px}.rich_media_meta.meta_tag_text{margin-right:0}.rich_media_meta_list em{font-style:normal}.rich_media_meta_text{color:#a5a5a5;}p{margin:0;}.msgBox{margin-top:20px;padding-top:20px;padding-left:50px;overflow:hidden;border-top:2px dashed #09a2ff;}.msg{padding-top:7px;clear:both;}.msgBody{float:right;width:100%;margin-left:55px;padding-bottom:15px;border-bottom:1px dashed #e0e0e0;}.userHeadImg{float:left;margin-left:-50px;}.userHeadImg img{width:40px;height:40px;margin-right:10px;border-radius:3px;}.userName{color:#888888;line-height:24px;font-size:14px;margin:5px 0 5px 0;height:24px;}.replyBody,.autherBody{color:#565656;font-size:15px;}.replyIcon{border-left:4px solid #33ab01;margin-right:5px;}.ad{text-decoration:none;color:#d6d4d4;font-size:12px;}.msgBodyReply{padding-top:5px;}.userName span{float:right;color:#afafaf;font-size:14px;}code{text-align:left;font-size:14px;display:block;white-space:pre;display:-webkit-box;display:-webkit-flex;display:flex;position:relative;}.code-snippet__fix{font-size:14px;margin:10px 0;display:block;color:#333;position:relative;background-color:rgba(0,0,0,0.03);border:1px solid #f0f0f0;border-radius:2px;display:-webkit-box;display:-webkit-flex;display:flex;padding-left:25px;line-height:26px}.code-snippet__fix code{text-align:left;font-size:14px;display:block;white-space:pre;display:-webkit-box;display:-webkit-flex;display:flex;position:relative;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace}.code-snippet__comment,.code-snippet__quote{color:#afafaf;font-style:italic}.code-snippet__keyword,.code-snippet__selector-tag,.code-snippet__subst{color:#ca7d37}.code-snippet__number,.code-snippet__literal,.code-snippet__variable,.code-snippet__template-variable,.code-snippet__tag .code-snippet__attr{color:#0e9ce5}.code-snippet__string,.code-snippet__doctag{color:#d14}.code-snippet__title,.code-snippet__section,.code-snippet__selector-id{color:#d14}.code-snippet__subst{font-weight:normal}.code-snippet__type,.code-snippet__class .code-snippet__title{color:#0e9ce5}.code-snippet__tag,.code-snippet__name,.code-snippet__attribute{color:#0e9ce5;font-weight:normal}.code-snippet__regexp,.code-snippet__link{color:#ca7d37}.code-snippet__symbol,.code-snippet__bullet{color:#d14}.code-snippet__built_in,.code-snippet__builtin-name{color:#ca7d37}.code-snippet__meta{color:#afafaf}.code-snippet__deletion{background:#fdd}.code-snippet__addition{background:#dfd}.code-snippet__emphasis{font-style:italic}.code-snippet__strong{font-weight:bold}.account_avatar{width:40px;height:40px;padding:0;}.account_info{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-box-align:center;-webkit-align-items:center;padding:20px 0;align-items:center}.flex_bd{padding-left:14px;}.account_nickname{display:inline-block;vertical-align:middle;line-height:1.2;color:#576b95;font-size:14px}.account_desc{overflow:hidden;text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:1;color:rgba(0,0,0,0.3);font-size:14px;line-height:1.2;padding-top:.4em}.msg_source_url{text-align:left;word-break:break-all;margin-top:20px;}.msg_source_url a{padding-right:10px;}.msg_source_url .url_text{color:#a8a8a8;}.video-desc{font-size:14px;margin-top:15px;color:#6c6c6c;}.msg_source_url{text-align:left;}.original_primary_card_tips{color:rgba(0,0,0,0.3);line-height:1.4;font-size:15px;}.weui-flex__item{margin-bottom:20px;padding:20px 16px;margin-top:16px;line-height:1.4;align-items:center;background-color:#f7f7f7;border-radius:8px;position:relative;}.original_primary_desc{color:rgba(0,0,0,0.5);font-size:14px;padding-top:4px;width:auto;overflow:hidden;text-overflow:ellipsis;}.msgBodyReplyList{border-top:1px solid #e1e1e1;margin-top:10px;}.msgBodyReplyListTop{border-top:0;}.reply_like_num{float:right;font-size:14px;color:#c7c7c7;}.msgData{margin-top:20px;color:#626262;}.msgData span{font-size:14px;padding-right:15px;}.msgData .likes{float:right;padding-right:0;}.js_text_content p{font-size:18px;}.rich_media_meta_link{font-size:15px;}blockquote {padding-left: 10px;border-left: 3px solid #dbdbdb;color: rgba(0,0,0,0.5);font-size: 15px;padding-top: 4px;margin: 1em 0;}.video_iframe{width:500px;height:400px;}.blockquote_info{color:#b5b5b5;margin-top:10px;}#copyright_logo{color:#bdbdbd;}.rich_media_meta_list{margin-bottom:10px;}.reprint{background:#efefef;border-radius:5px;padding:8px;color:#1f1f1f;}.reprint a{word-break:break-all;}.topic{color:#8e8e8e;background:#f7f7f7;border-radius:5px;padding:10px 8px;}.topic a{padding-right:5px;}.topic p{margin-bottom:5px;}
</style>
<link href="https://www.juyifx.cn/config/css/wxArticle.css" rel="stylesheet">
</head><script>

</script>
<body><p xmlns="http://www.w3.org/1999/xhtml" style='margin-right: 8px;margin-bottom: 0px;margin-left: 8px;outline: 0px;text-wrap: wrap;background-color: rgb(255, 255, 255);font-size: 14px;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;color: rgb(63, 63, 63);visibility: visible;'><span style='outline: 0px;color: rgb(136, 136, 136);font-family: "PingFang SC";letter-spacing: 0.1em;visibility: visible;'>作者：<span style="outline: 0px;letter-spacing: 0.1em;text-decoration-style: solid;text-decoration-color: rgb(136, 136, 136);visibility: visible;"><span style="color: rgb(136, 136, 136); font-size: 14px; letter-spacing: 0.1em; visibility: visible;">SEBASTIAN RASCHKA, PHD</span></span><br style="outline: 0px;visibility: visible;"></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='margin-right: 8px;margin-bottom: 0px;margin-left: 8px;outline: 0px;text-wrap: wrap;background-color: rgb(255, 255, 255);font-size: 14px;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;color: rgb(63, 63, 63);visibility: visible;'><span style='outline: 0px;color: rgb(136, 136, 136);font-family: "PingFang SC";letter-spacing: 1.4px;visibility: visible;'>编译：活水智能</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='margin-right: 8px;margin-bottom: 0px;margin-left: 8px;outline: 0px;text-wrap: wrap;background-color: rgb(255, 255, 255);font-size: 14px;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;color: rgb(63, 63, 63);visibility: visible;'><span style='outline: 0px;font-family: "PingFang SC";letter-spacing: 0.1em;color: rgb(136, 136, 136);visibility: visible;'>来源: https://magazine.sebastianraschka.com/p/understanding-large-language-models</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px; text-wrap: wrap; text-align: justify; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; margin: 1.5em 8px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;'>大语言模型以一种近乎风暴的方式占据了公众的视野——这里没有双关语。仅仅五年时间，这些模型——尤其是Transformer——已经彻底颠覆了自然语言处理领域。不仅如此，它们还在计算机视觉和计算生物学等领域引发了革命。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-family: "PingFang SC"; font-size: 14px; text-wrap: wrap; text-align: justify; line-height: 1.75; margin: 1.5em 8px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;'>鉴于Transformer对研究方向的深远影响，我整理了一份入门阅读清单，帮助机器学习的新手研究人员和从业者快速上<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; letter-spacing: 0.1em; visibility: visible;'>手。</span></p><h2 xmlns="http://www.w3.org/1999/xhtml" style='letter-spacing: normal; text-wrap: wrap; text-align: justify; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; font-size: 1.2em; font-weight: bold; display: table; margin: 4em auto 2em; padding-right: 0.2em; padding-left: 0.2em; background: rgb(15, 76, 129); color: rgb(255, 255, 255); visibility: visible;'>主要架构和任务的理解</h2><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px; text-wrap: wrap; text-align: justify; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; margin: 1.5em 8px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;'>如果你是Transformer或大语言模型的新手，从基础开始学习是最合理的。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px; text-wrap: wrap; text-align: justify; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; margin: 1.5em 8px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;'><strong style="line-height: 1.75; color: rgb(15, 76, 129); visibility: visible;">(1)</strong> <span style="font-style: italic; visibility: visible;"><strong style="line-height: 1.75; color: rgb(15, 76, 129); visibility: visible;">通过联合学习对齐和翻译的神经机器翻译</strong></span> (2014)，由Bahdanau, Cho和Bengio撰写，<span style="line-height: 1.75; color: rgb(87, 107, 149); visibility: visible;"><span style="color: rgb(87, 107, 149); font-size: 14px; letter-spacing: 1.4px; visibility: visible;">https://arxiv.org/abs/1409.0473</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px; text-wrap: wrap; text-align: justify; line-height: 1.75; font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; margin: 1.5em 8px; letter-spacing: 0.1em; color: rgb(63, 63, 63); visibility: visible;'>我建议从上述论文开始阅读，如果你能抽出几分钟的时间。它为循环神经网络(RNN)引入了一种注意力机制，以增强其长序列建模能力。这种机制使RNN能够更准确地翻译较长的句子，这也是后来开发原始Transformer架构的初衷。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg" data-imgfileid="100001576" data-ratio="0.43805309734513276" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT93beVibrwL3PdaUROzlnx13Mmt6ehibwAwZEgGh75Nhu6uWD9gAHZp1g/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="904" style="height: auto !important; visibility: visible !important; width: 677px !important;" data-original-style="" data-index="1" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT93beVibrwL3PdaUROzlnx13Mmt6ehibwAwZEgGh75Nhu6uWD9gAHZp1g/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片" data-fail="0"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/1409.0473</span><strong style="line-height: 1.75;color: rgb(15, 76, 129);"><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'></span></strong></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(2)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">只需注意力</strong></span> (2017)，由Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser和Polosukhin撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/1706.03762</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>上述论文首次提出了包含编码器和解码器的原始Transformer架构，这在后续将作为独立的模块出现。此外，这篇论文还引入了缩放点积注意力机制、多头注意力块和位置输入编码等核心概念，这些至今仍是现代Transformer的基石。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001577" data-ratio="0.837378640776699" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT4xXufZZnrwC0GLNRnZJjicSu9snJFX1xX5gyIoemIZ0KkPNHms4XU6g/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="824" style="width: 677px !important; height: 566.905px !important;" data-original-style="" data-index="2" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT4xXufZZnrwC0GLNRnZJjicSu9snJFX1xX5gyIoemIZ0KkPNHms4XU6g/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/1706.03762</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(3)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">Transformer架构中的层规范化</strong></span> (2020)，由Xiong, Yang, He, K Zheng, S Zheng, Xing, Zhang, Lan, Wang和Liu撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2002.04745</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>尽管原始的Transformer示意图（来自《只需注意力》，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/1706.03762</span></span>）很好地总结了原始的编解码器架构，但图中的LayerNorm位置仍有争议。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>例如，《只需注意力》中的Transformer图将层规范化放在残差块之间，这与原始论文附带的<span style="line-height: 1.75;">官方（更新后的）代码实现</span>不符。图中所示的变体被称为Post-LN Transformer，而更新的代码默认使用Pre-LN变体。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style="line-height: 1.75;">Transformer架构中的层规范化</span>论文认为Pre-LN表现更<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>佳，解决了梯度问题，如下所示。许多实际应用的架构采用了这种方式，但它可能导致表示能力下降。</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>因此，尽管关于使用Post-LN或Pre-LN的讨论仍在进行，也有一篇新论文提出了兼顾两者优点的方法：<span style="font-style: italic;">具有双重残差连接的Transformer</span> (<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2304.14802</span></span>)；其实用性还有待观察。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001578" data-ratio="0.4675925925925926" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTAsy818piaMAqT4mMIoBSic04qQHjfXybrzsQFcJywicrIrOntKdibWeBpA/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 316.56px !important;" data-original-style="" data-index="3" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTAsy818piaMAqT4mMIoBSic04qQHjfXybrzsQFcJywicrIrOntKdibWeBpA/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>资料来源：https://arxiv.org/abs/1706.03762（左和中）和 https://arxiv.org/abs/2002.04745（右）</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(4)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">学习控制快速权重存储器：动态循环神经网络的替代方案</strong></span> (1991)，由Schmidhuber撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://doi.org/10.1162/neco.1992.4.1.131</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这篇论文推荐给那些对历史小知识和早期方法（与现代Transformer基本相似）感兴趣的人。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>例如，在1991年，也就是在《只需注意力》原始Transformer论文发表前约二十五年半，Juergen Schmidhuber提出了一种循环神经网络的替代方案，称为快速权重编程器（FWP）。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这种方法涉及一个前馈神经网络，通过梯度下降慢慢学习来编程另一个神经网络的快速权重变化。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style="line-height: 1.75;">在这篇博客文章中</span>解释了与现代Transformer的类比如下：</p><blockquote xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;border-left: none;padding: 1em;border-radius: 8px;color: rgba(0, 0, 0, 0.5);background: rgb(247, 247, 247);margin: 2em 8px;'><p style='line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 1em;letter-spacing: 0.1em;color: rgb(80, 80, 80);text-align: justify;'><span style="font-style: italic;">在今天的Transformer术语中，FROM和TO被称为键和值。应用于快速网络的输入被称为查询。本质上，查询通过快速权重矩阵处理，该矩阵是键和值的外积之和（忽略了规范化和投影）。由于两个网络的所有操作都是可微的，我们通过加法外积或二阶张量积实现了端到端可微的快速权重变化的主动控制。[FWP0-3a] 因此，慢网可以通过梯度下降学习在序列处理期间迅速修改快速网。这在数学上等价（除了规范化）于后来所称的带线性化自注意力的Transformer（或线性Transformer）。</span></p></blockquote><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>正如上述博客文章所述，这种方法现在被称为“线性Transformer”或“带线性化自注意力的Transformer”，通过2020年在arXiv上出现的更多近期论文<span style="line-height: 1.75;">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</span> 和 <span style="line-height: 1.75;">Rethinking Attention with Performers</span>进行说明。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>2021年，<span style="line-height: 1.75;">线性Transformer实际上是快速权重编程器</span>论文则明确展示了线性化自注意力和1990年代的快速权重编程器之间的等价性。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: center;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001579" data-ratio="1.01131221719457" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT8cP1OKAMFnzNE7dNhtmlicnYxRvibMJqqo3KWJbE38yL3bE6QquQdSPg/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="884" style="width: 311px !important; height: 314.518px !important;" data-original-style="width: 311px;height: 315px;" data-index="4" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT8cP1OKAMFnzNE7dNhtmlicnYxRvibMJqqo3KWJbE38yL3bE6QquQdSPg/640?wx_fmt=webp&amp;from=appmsg" _width="311px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>资料来源：基于 https://people.idsia.ch//~juergen/fast-weight-programmer-1991-transformer.html#sec2 的注释图</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(5)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">通用语言模型微调用于文本分类</strong></span> (2018)，由Howard和Ruder撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/1801.06146</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这还是一篇从历史角度非常有趣的论文。虽然它是在原始的《只需注意力》Transformer发布一年后撰写的，但它并不涉及Transformer，而是关注循环神经网络。然而，它仍然值得注意，因为它有效地提出了<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>预训练语言模型和迁移学习的概念，用于下游任务。</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>尽管迁移学习已经在计算机视觉领域确立，但在自然语言处理（NLP）领域还不普遍。ULMFit是最早的一些论文之一，展示了预训练语言模型并在特定任务上进行微调，可以在许多NLP任务中获得最先进的结果。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>ULMFit建议的语言模型微调三阶段过程如下：</p><ul xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;list-style: circle;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">• 在大量文本语料库上训练语言模型。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">• 在任务特定数据上微调这个预训练的语言模型，使其适应文本的特定风格和词汇。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">• 在任务特定数据上微调分类器，并逐渐解冻层以避免灾难性遗忘。</p></li></ul><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这种配方——在大型语料库上训练语言模型，然后在下游任务上进行微调——是基于Transformer的模型和基础模型，如BERT、GPT-2/3/4、RoBERTa等的核心方法。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>然而，ULMFiT中的逐渐解冻，通常不在使用Transformer架构时常规进行，其中所有层通常同时进行微调。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001580" data-ratio="0.4824074074074074" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT0Y1lb9pSvKibrbjibZoSBic2TcYHrficBictT7LQiajgxcff7iahnibOaLgQwg/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 499px !important; height: 240.721px !important;" data-original-style="width: 499px;height: 241px;" data-index="5" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT0Y1lb9pSvKibrbjibZoSBic2TcYHrficBictT7LQiajgxcff7iahnibOaLgQwg/640?wx_fmt=webp&amp;from=appmsg" _width="499px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: center;"><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/1801.06146</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(6)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">BERT: 双向Transformer的语言理解预训练</strong></span> (2018)，由Devlin, Chang, Lee和Toutanova撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/1810.04805</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>在原始Transformer架构之后，大语言模型研究开始向两个方向分化：用于预测建模任务（如文本分类）的编码器风格Transformer和用于生成建模任务（如翻译、摘要和其他形式的文本创作）的解码器风格Transformer。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>上述BERT论文介绍了原始的遮蔽语言模型和下一句预测的概念。它仍然是最有影响力的编码器风格架构。如果你对这个研究分支感兴趣，我建议你继续关注<span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 0.1em;text-decoration: none;'>RoBERTa</span>，它通过删除下一句预测任务简化了预训练目标。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001581" data-ratio="0.6064814814814815" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTFoU48XOMQC2Cz3K01G2wf2JoJZPSXgosMxkia0TPh2NMic6tsN6Xtxhg/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 525px !important; height: 318.403px !important;" data-original-style="width: 525px;height: 318px;" data-index="6" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTFoU48XOMQC2Cz3K01G2wf2JoJZPSXgosMxkia0TPh2NMic6tsN6Xtxhg/640?wx_fmt=webp&amp;from=appmsg" _width="525px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/1810.04805</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(7)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">通过生成预训练提高语言理解</strong></span> (2018)，由Radford和Narasimhan撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);">https://openai.com/research/language-unsupervised.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>原始GPT论文介绍了流行的解码器风格架构和通过下一个词预测进行预训练的方法。由于其遮蔽语言模型预训练目标，BERT可以被视为双向Transformer，而GPT是一个单向的、自回归模型。虽然GPT嵌入也可以用于分类，但GPT方法是当今最有影响力的LLM（如chatGPT）的核心。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>如果你对这个研究分支感兴趣，我建议你继续关注<span style="line-height: 1.75;">GPT-2</span>和<span style="line-height: 1.75;">GPT-3</span>论文。这两篇论文说明了LLM能够进行零样本和少样本学习，并突出了LLM的紧急能力。GPT-3还是目前流<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>行的基线和当前一代LLM（如ChatGPT）训练的基础模型——稍后我们将单独介绍导致ChatGPT的InstructGPT方法。</span></p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001582" data-ratio="0.5" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTFIYW7iaySBmpmq4kCdNqPibpS5vQ2W8WfT2gX8uJR9BH2HicflibnyKsHw/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 528px !important; height: 264px !important;" data-original-style="width: 528px;height: 264px;" data-index="7" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTFIYW7iaySBmpmq4kCdNqPibpS5vQ2W8WfT2gX8uJR9BH2HicflibnyKsHw/640?wx_fmt=webp&amp;from=appmsg" _width="528px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(8)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">BART: 用于自然语言生成、翻译和理解的去噪序列到序列预训练</strong></span> (2019)，由Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov和Zettlemoyer撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/1910.13461</span></span>.</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>如前所述，BERT类型的编码器风格LLM通常更适用于预测建模任务，而GPT类型的解码器风格LLM更擅长生成文本。为了融合两者的优势，上述BART论文结合了编码器和解码器部分（不像原始的Transformer——本列表的第二篇论文）。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001584" data-ratio="0.4302788844621514" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTLwtMwQpve1AVSpjpxIYciapwe757FpsVqwXW1kFXYyRIjhNeEdmchsQ/640?wx_fmt=other&amp;from=appmsg" data-type="webp" data-w="1004" style="width: 677px !important; height: 291.299px !important;" data-original-style="" data-index="8" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTLwtMwQpve1AVSpjpxIYciapwe757FpsVqwXW1kFXYyRIjhNeEdmchsQ/640?wx_fmt=other&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/1910.13461</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(9)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">在实践中利用LLM的力量：关于ChatGPT及其它的调查</strong></span> (2023)，由Yang, Jin, Tang, Han, Feng, Jiang, Yin和Hu撰写，<span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2304.13712</span></span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这不是一篇研究论文，但可能是迄今为止最好的总体架构调查，展示了不同架构的演变。然而，除了讨论BERT风格的遮蔽语言模型（编码器）和GPT风格的自回归语言模型（解码器）之外，它还提供了有关预训练和微调数据的有用讨论和指导。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001585" data-ratio="0.7722222222222223" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTPooAMEIcVQX6YuhGibRjMcjATdWWL4ymB8YCLic0RGslic7DBwr1TASLw/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style='font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif; font-size: var(--articleFontsize); letter-spacing: 0.034em; width: 677px !important; height: 522.794px !important;' data-original-style='font-family: mp-quote, -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: var(--articleFontsize);letter-spacing: 0.034em;' data-index="9" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTPooAMEIcVQX6YuhGibRjMcjATdWWL4ymB8YCLic0RGslic7DBwr1TASLw/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>现代LLMs的进化树 via https://arxiv.org/abs/2304.13712.</span></p><h2 xmlns="http://www.w3.org/1999/xhtml" style='letter-spacing: normal;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 1.2em;font-weight: bold;display: table;margin: 4em auto 2em;padding-right: 0.2em;padding-left: 0.2em;background: rgb(15, 76, 129);color: rgb(255, 255, 255);'>提高效率的比例定律</h2><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>如果你想了解更多关于提高Transformer效率的各种技术，请查阅<span style="line-height: 1.75;">2020</span>年的<span style="line-height: 1.75;">高效Transformer：一项调查</span>论文（https://arxiv.org/abs/2009.06732），然后是<span style='color: rgb(63, 63, 63);font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;font-size: 14px;letter-spacing: 0.1em;text-decoration: none;'>2023年的Transformer高效训练的调查论文</span>（https://arxiv.org/abs/2302.01107）。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>此外，下面是我发现特别有趣且值得一读的论文。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(10)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">FlashAttention: 具有IO感知的快速和内存高效的精确注意力</strong></span> (2022), 由Dao, Fu, Ermon, Rudra和Ré撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2205.14135</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>虽然大多数Transformer论文不考虑替换原始的缩放点积机制来实现自注意力，但FlashAttention是我最近经常看到的一种机制。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001586" data-ratio="0.5749063670411985" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTicG0AjDQrIPd8TpGJunRJhbySicaMqkcslkM0DWRTLBVOib4LyxAMcSsQ/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1068" style="width: 677px !important; height: 389.212px !important;" data-original-style="" data-index="10" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTicG0AjDQrIPd8TpGJunRJhbySicaMqkcslkM0DWRTLBVOib4LyxAMcSsQ/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2205.14135</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(11)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">Cramming: 在一天内在单个GPU上训练语言模型</strong></span> (2022) 由Geiping和Goldstein撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2212.14034</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>在这篇论文中，研究人员在单个GPU上训练了一个遮蔽语言模型/编码器风格的LLM（这里是BERT）24小时。相比之下，原始的2018年BERT论文在16个TPU上训练了四天。一个有趣的见解是，虽然较小的模型有更高的吞吐量，但较小的模型也学习得不那么高效。因此，较大的模型不需要更多的训练时间就可以达到特定的预测性能阈值。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001587" data-ratio="0.4722222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTlgS7HstiaS3Gvt42JheEscoYp6gKxwY0eUQgJvQZWd9iaGOLmpicUibIQg/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 319.694px !important;" data-original-style="" data-index="11" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTlgS7HstiaS3Gvt42JheEscoYp6gKxwY0eUQgJvQZWd9iaGOLmpicUibIQg/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2212.14034</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(12)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">LoRA：大型语言模型的低秩调整</strong></span> (2021) 由Hu, Shen, Wallis, Allen-Zhu, Li, L Wang, S Wang和Chen撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2106.09685</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>现代大型语言模型在大型数据集上进行预训练，在各种任务上表现出色，包括语言翻译、摘要、编码和问答。然而，如果我们想提高Transformer在特定领域数据和专业任务上的能力，微调Transformer是值得的。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>低秩调整（LoRA）是微调大型语言模型中最有影响力的方法之一。虽然存在其他参数高效的微调方法（见下面的调查），但LoRA特别值得一提，因为它既优雅又通用，可以应用于其他类型的模型。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>虽然预训练模型在预训练任务上的权重具有完全秩，但LoRA作者指出，当它们适应新任务时，预训练的大型语言模型具有低“内在维度”。因此，LoRA的主要思想是将权重变化ΔW分解为更参数高效的低秩表示。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001588" data-ratio="0.3148148148148148" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTT569GLbXlARDPAebAwulrI6rKbJBaFDBZicec5jv8L6zosQGibqMW3ew/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 213.13px !important;" data-original-style="" data-index="12" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTT569GLbXlARDPAebAwulrI6rKbJBaFDBZicec5jv8L6zosQGibqMW3ew/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style=";"><span style=";"><span style=";">LoRA 及其性能的插图 https://arxiv.org/abs/2106.09685</span></span></span><span style=";">.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(13)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">通过缩小规模来提升规模：参数高效微调的指南</strong></span> (2022) 由Lialin, Deshpande和Rumshisky撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2303.15647</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>现代大型语言模型在大型数据集上进行预训练，在各种任务上表现出色，包括语言翻译、摘要、编码和问答。然而，如果我们想提高Transformer在特定领域数据和专业任务上的能力，微调Transformer是值得的。这项调查回顾了40多篇关于参数高效微调方法的论文（包括流行的技术，如前缀调整、适配器和低秩调整），使微调（非常）计算高效。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001589" data-ratio="0.512962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTe9KpZicic7oJYKFsEicq6MOiaFN9h1vOaTTl0oh3UKrWKleDLcVYeEGHgg/640?wx_fmt=other&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 347.276px !important;" data-original-style="" data-index="13" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTe9KpZicic7oJYKFsEicq6MOiaFN9h1vOaTTl0oh3UKrWKleDLcVYeEGHgg/640?wx_fmt=other&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2303.15647</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(14)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">规模化语言模型：方法、分析及Gopher训练的见解</strong></span> (2022) 由Rae和同事（78名合著者！）撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2112.11446</span>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>Gopher是一篇特别好的论文，包括大量分析，以了解LLM训练。在这里，研究人员训练了一个拥有2800亿参数和80层的模型，在3000亿token上进行训练。这包括一些有趣的架构修改，例如使用RMSNorm（均方根标准化）而不是LayerNorm（层正则化）。由于它们不依赖于批量大小，并且不需要同步，所以LayerNorm和RMSNorm优于BatchNorm，这在分布式设置中使用较小批量时是一个优势。然而，通常认为RMSNorm可以在更深的架构中稳定训练。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>除了上述有趣的细节之外，这篇论文的主要焦点是对不同规模的任务性能的分析。在152个不同任务的评估中，增加模型大小最有益于理解、事实核查和识别有害语言的任务。然而，与逻辑和数学推理相关的任务从架构扩展中受益较少。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001590" data-ratio="0.787962962962963" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT9VS6ic71Ne5BOXyaZmOPiclLsd1MKs5CD7gkM2AVx6TVfypzWcOq8VAw/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 533.451px !important;" data-original-style="" data-index="14" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT9VS6ic71Ne5BOXyaZmOPiclLsd1MKs5CD7gkM2AVx6TVfypzWcOq8VAw/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>资料来源：图来自 https://arxiv.org/abs/2112.11446</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(15)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">训练计算最优的大型语言模型</strong></span> (2022) 由Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de Las Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals和Sifre撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2203.15556</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这篇论文介绍了70亿参数的Chinchilla模型，它在生成建模任务上超越了流行的1750亿参数的GPT-3模型。然而，它的主要亮点是，当代大型语言模型“显著低训练”。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这篇论文定义了大型语言模型训练的线性比例定律。例如，虽然Chinchilla只有GPT-3的一半大小，但它超越了GPT-3，因为它在1.4万亿（而不是仅3000亿）token上进行了训练。换句话说，训练token的数量与模型大小一样重要。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001591" data-ratio="0.6222222222222222" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTJEQnEC8QiaTgEHHj8jwtBkEGpJKe9kpiaP3CibhKAialzd3MkegeicJlk1g/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 421.244px !important;" data-original-style="" data-index="15" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTJEQnEC8QiaTgEHHj8jwtBkEGpJKe9kpiaP3CibhKAialzd3MkegeicJlk1g/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2203.15556</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(16)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">Pythia</strong></span><strong style="line-height: 1.75;color: rgb(15, 76, 129);">:</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">分析大型语言模型在训练和规模化过程中的套件</strong></span> (2023) 由Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika和van der Wal撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2304.01373</span>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>Pythia是一套开放源码的LLM（从7000万到120亿参数），用于研究LLM在训练过程中的演变。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>该架构类似于GPT-3，但包括一些改进，例如Flash Attention（如<span style='color: rgb(63, 63, 63);font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 14px;letter-spacing: 1.4px;text-align: left;text-wrap: wrap;'></span><span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(63, 63, 63);'>LLaMA</span><span style="line-height: 1.75;color: rgb(87, 107, 149);"></span>）和旋转位置嵌入（如<span style='color: rgb(63, 63, 63);font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 14px;letter-spacing: 1.4px;text-align: left;text-wrap: wrap;'><span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(63, 63, 63);'>PaLM</span></span>）。Pythia在<span style='color: rgb(63, 63, 63);font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 14px;letter-spacing: 1.4px;text-align: left;text-wrap: wrap;'><span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(63, 63, 63);'>The Pile数据集</span></span>（825 Gb）上训练了3000亿token（<span style="text-decoration:line-through;">1个epoch在常规PILE上，</span>1.5个epoch在去重的PILE上）。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001592" data-ratio="0.3490740740740741" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTK2iaWCOBJTpu6fCy8SZedrm2R3iaqcnRrjicy7zhvcwuEEWgViaNmHnmbw/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 236.323px !important;" data-original-style="" data-index="16" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTK2iaWCOBJTpu6fCy8SZedrm2R3iaqcnRrjicy7zhvcwuEEWgViaNmHnmbw/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style=";"><span style=";"><span style=";">通过 https://arxiv.org/abs/2304.01373 的Pythia模型套件</span></span></span><span style=";">.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>Pythia研究的主要见解如下：</p><ol xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">1. 在重复数据上训练（由于LLM的训练方式，这意味着训练超过一个epoch）既不利于也不损害性能。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">2. 训练顺序不影响记忆。这很不幸，因为如果情况相反，我们可以通过重新排序训练数据来减轻不希望的逐字记忆问题。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">3. 预训练术语频率影响任务性能。例如，对于更常见的术语，少样本准确性往往更高。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">4. 加倍批量大小可以将训练时间减半，但不会影响收敛。</p></li></ol><h2 xmlns="http://www.w3.org/1999/xhtml" style='letter-spacing: normal;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 1.2em;font-weight: bold;display: table;margin: 4em auto 2em;padding-right: 0.2em;padding-left: 0.2em;background: rgb(15, 76, 129);color: rgb(255, 255, 255);'>对齐——引导大型语言模型实现预期目标和兴趣</h2><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>近年来，我们看到了许多相对有能力的大型语言模型，它们可以生成现实的文本（例如，GPT-3和Chinchilla等）。看来我们已经达到了常用预训练范式所能达到的上限。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>为了使语言模型更有帮助并减少错误信息和有害语言，研究人员设计了额外的训练范式来微调预训练的基础模型。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(17)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">训练语言模型遵循人类反馈的指令</strong></span> (2022) 由Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike和Lowe撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2203.02155</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>在所谓的InstructGPT论文中，研究人员使用了一种带有人类回路的强化学习机制（RLHF）。他们从预训练的GPT-3基<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>础模型开始，然后使用人类生成的提示-响应对进行监督学习（第一步）。</span><span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>接下来，他们要求人类对模型输出进行排序，以训练奖励模型（第二步）。</span><span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>最后，他们使用奖励模型通过近端策略优化（proximal policy optimization）更新预训练和微调过的GPT-3模型（第三步）。</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>顺便说一句，这篇论文也被称为描述ChatGPT背后思想的论文——根据最近的传言，ChatGPT是InstructGPT的放大版本，已在更大的数据集上进行了微调。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001593" data-ratio="0.6351851851851852" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTE4ZccHSQM0gd8u5lJ2qWhUniaOLTvomtolcZUR5jSp6eaOqfPfdiaF8w/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 430.02px !important;" data-original-style="" data-index="17" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTE4ZccHSQM0gd8u5lJ2qWhUniaOLTvomtolcZUR5jSp6eaOqfPfdiaF8w/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2203.02155</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(18)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">宪法AI：来自AI反馈的无害性</strong></span> (2022) 由Yuntao, Saurav, Sandipan, Amanda, Jackson, Jones, Chen, Anna, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, El Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, Kaplan, <span style="line-height: 1.75;color: rgb(87, 107, 149);"></span><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2212.08073</span><span style='font-family: Söhne, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, Ubuntu, Cantarell, "Noto Sans", sans-serif, "Helvetica Neue", Arial, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";text-align: start;white-space: pre-wrap;color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;text-decoration: none solid rgb(87, 107, 149);'>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>在这篇论文中，研究人员将对齐理念推向更远，提出了一种创建“无害”AI系统的训练机制。与上述InstructGPT论文不同，研究人员提出了一种基于规则列表（由人类提供）的自我训练机制。与上述InstructGPT论文类似，所提出的方法使用了一种强化学习方法。</p><p xmlns="http://www.w3.org/1999/xhtml" style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001594" data-ratio="0.42777777777777776" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT9Vuxw52jwEY9f5n11YCZnwIicuYrvtW64BbQ1iasiaCicGSer54qXYv1sA/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 677px !important; height: 289.606px !important;" data-original-style="" data-index="18" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXT9Vuxw52jwEY9f5n11YCZnwIicuYrvtW64BbQ1iasiaCicGSer54qXYv1sA/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: center;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style='color: rgb(134, 135, 135);font-family: "SF Pro Display", -apple-system, system-ui, "system-ui", Inter, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";font-size: 14px;letter-spacing: -0.15px;text-align: center;text-wrap: wrap;background-color: rgb(255, 255, 255);'>来源：https://arxiv.org/abs/2212.08073</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">(19)</strong> <span style="font-style: italic;"><strong style="line-height: 1.75;color: rgb(15, 76, 129);">自我指令：通过自我生成的指令与语言模型对齐</strong></span> (2022) 由Wang, Kordi, Mishra, Liu, Smith, Khashabi和Hajishirzi撰写, <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: 1.4px;">https://arxiv.org/abs/2212.10560</span>.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>指令微调是我们如何从类似GPT-3的预训练基础模型转变为更有能力的LLM，如ChatGPT。而开源的人类生成的指令数据集，如<span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 0.1em;text-decoration: none;'>databricks-dolly-15k</span>，可以帮助实现这一目标。但我们如何扩大规模呢？一种方法是利用LLM自身的生成进行引导。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>Self-Instruct是一种（几乎无需注释的）方式，可以将预训练的LLM与指令对齐。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>这是如何工作的？简而言之，这是一个四步过程：</p><ol xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">1. 使用一组人类编写的指令（本例中为175条）和样本指令填充任务池。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">2. 使用预训练的LLM（如GPT-3）确定任务类别。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">3. 根据新的指令，让预训练的LLM生成响应。</p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;">4. 收集、修剪和过滤响应，然后将它们添加到任务池中。</p><p style="text-align: justify;"><br></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p style="text-align: justify;"><img class="rich_pages wxw-img js_insertlocalimg js_img_placeholder wx_img_placeholder" data-imgfileid="100001595" data-ratio="0.4527777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTPdrJaNkeKxgQbmLVev1FfVT5VmmUsZdDP38CFtz0wQAFGx2iaHU1UqA/640?wx_fmt=webp&amp;from=appmsg" data-type="webp" data-w="1080" style="width: 647px !important; height: 292.947px !important;" data-original-style="" data-index="19" src="https://mmbiz.qpic.cn/mmbiz_jpg/Oh47rXadcrfYiaZR2OUBaXekJ3oKib5RXTPdrJaNkeKxgQbmLVev1FfVT5VmmUsZdDP38CFtz0wQAFGx2iaHU1UqA/640?wx_fmt=webp&amp;from=appmsg" _width="677px" crossorigin="anonymous" alt="图片"></p></li></ol><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><span style=";"><span style=";"><span style=";">来自 https://arxiv.org/abs/2212.10560 的 self-instruct 方法的注释版本</span></span></span><span style=";">.</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>实际上，这种方法基于ROUGE分数表现相对良好。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>例如，经过Self-Instruct微调的LLM优于基础LLM（1），并且可以与在大型人类编写的指令集上预训练的LLM竞争（2）。Self-Instruct还可以使已经在人类指令上进行过微调的LLM受益（3）。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>但当然，评估LLM的黄金标准是询问人<span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>类评估者。</span><span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>根据人类评估，Self-Instruct优于基础LLM和在监督方式下训练的LLM（SuperNI, T0 Trainer）。</span><span style='font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;letter-spacing: 0.1em;'>但有趣的是，Self-Instruct在性能上不及通过人类反馈的强化学习（RLHF）训练的方法。</span></p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>人类生成的指令数据集和自我指令数据集哪个更有前景？我选择两者。为什么不从<span style='font-family: -apple-system-font, "system-ui", "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;text-align: left;text-wrap: wrap;color: rgb(63, 63, 63);font-size: 14px;letter-spacing: 0.1em;text-decoration: none;'>databricks-dolly-15k</span>这样的人类生成的指令数据集开始，然后用self-instruct来扩展呢？</p><h2 xmlns="http://www.w3.org/1999/xhtml" style='letter-spacing: normal;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;font-size: 1.2em;font-weight: bold;display: table;margin: 4em auto 2em;padding-right: 0.2em;padding-left: 0.2em;background: rgb(15, 76, 129);color: rgb(255, 255, 255);'>结论和进一步阅读</h2><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>我试图保持上述列表简洁明了，重点介绍前10篇论文（加上3篇有关RLHF的额外论文），以理解当代大语言模型的设计、限制和演变。</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: justify;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'>如需进一步阅读，我建议您参考上述论文中的参考文献。或者，为了给您一些额外的指南，这里是一些额外的资源（这些列表不是全面的）：</p><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">GPT的开源替代品</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;list-style: circle;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">BLOOM：一个1760亿参数的开放获取多语言语言模型</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2211.05100</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">OPT：开放预训练的Transformer语言模型</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2205.01068</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">UL2：统一语言学习范式</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2205.05131</span></span></p></li></ul><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">ChatGPT的替代品</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;list-style: circle;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">LaMDA：对话应用的语言模型</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2201.08239</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• (Bloomz) <span style="font-style: italic;">通过多任务微调实现跨语言泛化</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2211.01786</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• (Sparrow) <span style="font-style: italic;">通过有针对性的人类判断改进对话代理的对齐</span> (2022), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2209.14375</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">BlenderBot 3: 部署的对话代理，不断学习负责任地参与</span> <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2208.03188</span></span></p></li></ul><p xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;margin: 1.5em 8px;letter-spacing: 0.1em;color: rgb(63, 63, 63);'><strong style="line-height: 1.75;color: rgb(15, 76, 129);">计算生物学中的大语言模型</strong></p><ul xmlns="http://www.w3.org/1999/xhtml" style='font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;padding-left: 1em;list-style: circle;color: rgb(63, 63, 63);' class="list-paddingleft-1"><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">ProtTrans：通过自我监督的深度学习和高性能计算破解生命代码的语言</span> (2021), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://arxiv.org/abs/2007.06225</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">AlphaFold：高精度的蛋白质结构预测</span> (2021), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://www.nature.com/articles/s41586-021-03819-2</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p>• <span style="font-style: italic;">大型语言模型在多样化的蛋白质家族中生成功能性蛋白质序列</span> (2023), <span style="line-height: 1.75;color: rgb(87, 107, 149);"><span style="color: rgb(87, 107, 149);font-size: 14px;letter-spacing: normal;">https://www.nature.com/articles/s41587-022-01618-2</span></span></p></li><li style="text-align: left;line-height: 1.75;text-indent: -1em;display: block;margin: 0.2em 8px;"><p><br></p></li></ul><section xmlns="http://www.w3.org/1999/xhtml" class="mp_profile_iframe_wrp"><mp-common-profile class="js_uneditable custom_select_card mp_profile_iframe mp_common_widget js_wx_tap_highlight" data-pluginname="mpprofile" data-id="Mzk0OTY0NzM1Ng==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/Oh47rXadcrdibxALa66icAMaGzPic3wRPiaS3iccsMu3UTBjtHiamZMlAWcf1icpRJKqEsjLQcGfMcBnD1nlUQZVickHxg/300?wx_fmt=png&amp;wxfrom=19" data-nickname="活水智能" data-alias="" data-signature="AI生产力专家。致力于通过AI教育、软件及社群提高专业知识工作者的生产力。 官网：huoshuiai.com" data-from="0" data-is_biz_ban="0" data-origin_num="61" data-isban="0" data-biz_account_status="0" data-index="0"></mp-common-profile></section><hr xmlns="http://www.w3.org/1999/xhtml" style='color: rgb(0, 0, 0);font-size: 14px;letter-spacing: normal;text-wrap: wrap;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;border-style: solid;border-right-width: 0px;border-bottom-width: 0px;border-left-width: 0px;border-color: rgba(0, 0, 0, 0.1);transform-origin: 0px 0px;transform: scale(1, 0.5);'><h4 xmlns="http://www.w3.org/1999/xhtml" style='margin: 2em 8px 0.5em;outline: 0px;font-weight: bold;font-size: 1em;letter-spacing: normal;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;color: rgb(1, 155, 252);'>推荐阅读</h4><ul xmlns="http://www.w3.org/1999/xhtml" class="list-paddingleft-1" style='padding-left: 1em;outline: 0px;font-size: 14px;letter-spacing: normal;text-align: left;line-height: 1.75;font-family: -apple-system-font, BlinkMacSystemFont, "Helvetica Neue", "PingFang SC", "Hiragino Sans GB", "Microsoft YaHei UI", "Microsoft YaHei", Arial, sans-serif;list-style: circle;color: rgb(63, 63, 63);'><li style="margin: 0.2em 8px;outline: 0px;text-align: left;line-height: 1.75;text-indent: -1em;display: block;"><p style="outline: 0px;text-align: justify;">• <a target="_blank" href="http://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&amp;mid=2247485169&amp;idx=1&amp;sn=cd8f4d1be87702fdec14dfea200bd824&amp;chksm=c3546462f423ed7479b2eb8455acf22c0913b7e1601e45fc3be29b0b0c8985bc2e83e91ef881&amp;scene=21#wechat_redirect" textvalue="AI是怎么学会绘画的？人人都能懂的扩散模型科普‍" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">AI是开源大模型太多怎么选？一文读懂，5个最好的开源大模型！</a></p></li><li style="margin: 0.2em 8px;outline: 0px;text-align: left;line-height: 1.75;text-indent: -1em;display: block;"><p style="outline: 0px;text-align: justify;">• <a href="https://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&amp;mid=2247485092&amp;idx=1&amp;sn=a3278a0697ee3042783d3f46e9479d38&amp;scene=21#wechat_redirect" title="让AI模仿作家写作风格！3个你不知道的技巧" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);cursor: pointer;line-height: 1.75;">让AI模仿作家写作风格！3个你不知道的技巧</a></p></li><li style="margin: 0.2em 8px;outline: 0px;text-align: left;line-height: 1.75;text-indent: -1em;display: block;"><p style="outline: 0px;text-align: justify;">• <a href="https://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&amp;mid=2247485004&amp;idx=1&amp;sn=19e003168f8f6280854f83857e3bdb82&amp;scene=21#wechat_redirect" title="用AI重塑新闻网站后，我总结了十个AI使用心得" data-linktype="2" style="outline: 0px;color: var(--weui-LINK);cursor: pointer;line-height: 1.75;">用AI重塑新闻网站后，我总结了十个AI使用心得</a></p></li></ul><p xmlns="http://www.w3.org/1999/xhtml" style="display: none;"><mp-style-type data-value="3"></mp-style-type></p></body>
</html>