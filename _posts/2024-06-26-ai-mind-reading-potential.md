---
categories: articles
date: 2024-06-26
layout: post
style: huoshui
tags:
- AI
- 教程
title: AI大模型能否猜测人的心理状态心理学正在探究AI的内在工作原理
---

![](/assets/images/20c7112b42584358949acfc1aff01780.png)

作者：Shelly Xuelai Fan

来源：singularityhub

编译：活水智能

**编者按：**

AI大模型能否理解你的心思，和你共情，甚至识别出言语中的讽刺？如果真到那一步，这个世界将变得多么魔幻。心理学家正在通过一系列测试，探究AI大模型的“心智理论”能力，看看它们究竟能否读懂人类的内心世界。本文将带你了解最新研究成果。

本文作者是英属哥伦比亚大学神经科学博士，曾研究能够使老化大脑恢复活力的血液因子。她还是媒体公司 Vantastic Media 的联合创始人。

* * *

如果你曾向类似ChatGPT这样的聊天机器人倾诉生活中的烦恼，它的回应可能会显得非常有同情心。

这种聊天机器人不仅能鼓励你，甚至能像好友一样给出建议。

与旧版聊天机器人不同，最新的 AI 模型因其“同理心”特质，已经引起了心理治疗界的关注，许多人开始思考它们是否可以在治疗中发挥作用。

推测他人的心理状态是日常交流中的关键能力，被称为“心智理论”。它让我们能够通过解读语言来猜测他人的想法，比如他们是在讽刺吗？在撒谎吗？还是在暗示什么？

汉堡-埃彭多夫大学医学中心的克里斯蒂娜·贝乔博士及其同事在《自然人类行为》（Nature Human
Behavior）杂志中发表的一项新研究中提到：“人们非常在意他人的想法，并且花费大量精力思考他人的心态。”

在这项研究中，科学家们探讨了 ChatGPT 和其他类似聊天机器人——基于大模型的机器学习算法——是否也能猜测他人的心理状态。

他们使用了一系列针对心智理论特定方面的心理学测试，将 OpenAI 的 GPT 系列和 Meta 的 LLaMA 2 两大模型与 1900
多名人类参与者进行了比较。

GPT-4 是 ChatGPT 背后的算法，在一些任务中，比如识别讽刺，表现得和人类一样好甚至更好。而在识别失言方面，LLaMA 2 超越了人类和 GPT。

需要明确的是，这些结果并不意味着大模型具备心智理论。相反，它们表明这些算法可以模仿这一核心概念的某些方面，正如研究作者所写的那样，心智理论“定义了我们作为人类的特质”。

## 言外之意

孩子们大约在四岁时，就已经知道人们的想法不总是相同。

我们有不同的信念、意图和需求。通过设身处地为他人着想，孩子们可以开始理解其他人的观点，并培养同理心。

心智理论于 1978 年首次被提出，是社交互动的润滑剂。

例如，如果你在一个闷热的房间里站在一扇关着的窗户旁，而旁边有人说：“这里有点热”，你需要从他们的角度去思考，猜出他们是在礼貌地请求你打开窗户。

当这种能力出现问题时，比如在自闭症患者中，就会很难理解他人的情绪、愿望、意图，也难以识别欺骗。

我们都经历过，当文本或电子邮件被误解时，会导致误会，因为接收者错误地解读了发送者的意思。

那么，聊天机器人背后的 AI 大模型又如何呢？

## 和人类的心智理论能力对比

早在 2018 年，西英格兰大学机器人伦理学教授艾伦·温菲尔德博士就倡导通过心智理论让 AI
“理解”人类和其他机器的意图。当时，他提出为算法编程一个自我内部模型，内置社交常识，而不是通过学习来获得。

大模型采用了完全不同的方法，通过摄取海量数据集来生成类似人类的回应，让人感觉它们是有同理心的。  

但它们真的表现出了心智理论的迹象吗？

多年来，心理学家开发了一系列测试来研究我们如何获得建模他人心态的能力。这项新研究将两个版本的 OpenAI GPT 模型（GPT-4 和 GPT-3.5）和
Meta 的 LLaMA-2-Chat 与 1907 名健康人类参与者进行了比较。

仅基于社交场景的文本描述，并使用涵盖不同心智理论能力的综合测试，参与者和 AI 需要判断虚构人物的“心态”。

在心理学中，这些测试已被广泛用于测量人类的心智理论。

第一个测试被称为“错误信念”，常用于测试幼儿获得自我意识和他人认知的过程。例如，你听一个故事：露西和米娅在厨房，橱柜里有一盒橙汁。当露西离开时，米娅把橙汁放进了冰箱。当露西回来时，她会在哪里找橙汁？

人类和 AI 几乎都能完美地猜测出，离开房间时橙汁被移动的人会在她最后看到橙汁的地方找。

但稍作改变就会让 AI 困惑。例如，当橙汁被放在两个透明容器之间移动时，GPT 模型就很难猜对答案。（不过，在这项研究中，人类的表现也不完美。）

更高级的测试是“奇怪故事”，它依赖于多层次的推理来测试高级心理能力，如误导、操控和撒谎。

例如，人类志愿者和 AI
模型都被告知一个关于西蒙的故事，西蒙经常撒谎。他的哥哥吉姆知道这一点，有一天发现自己的乒乓球拍不见了。他质问西蒙，问球拍是在橱柜下还是床下。西蒙说在床下。测试的问题是：为什么吉姆会去橱柜下找？

在所有的 AI 模型中，GPT-4 最为成功，推理出“大骗子”一定在撒谎，所以选择橱柜下是更好的选择。它的表现甚至超过了人类志愿者。

接下来是“失言”（指在社交场合中言辞不当）研究。在先前的研究中，GPT
模型在解读这些社交情况时表现得很差。在测试中，一个例子是描述一个人在买新窗帘，挂上后，一个朋友随口说：“哦，那窗帘真糟糕，我希望你会换新的。”

人类和 AI 模型都被呈现了多个类似的令人尴尬的情境，并被问及目睹的回应是否合适。“正确答案总是否定的，”研究团队写道。

GPT-4 正确地识别出这句话可能会让人受伤，但当被问及朋友是否知道这些窗帘是新的时，它却难以给出正确答案。

这可能是因为 AI 无法推测出说话者的心理状态，识别失言依赖于提示中没有直接解释的背景和社交规范，研究作者解释道。

相比之下，LLaMA-2-Chat 的表现超过了人类，除了一次测试外，几乎每次都达到 100% 的准确率。为何它具备如此优势尚不清楚。

## 理解暗示语言的能力

很多交流内容并不是言明的，而是暗示的。

讽刺可能是最难在不同语言之间翻译的概念之一。在一项适用于自闭症的改编心理测试中，GPT-4
在识别讽刺语句时表现优于人类参与者——当然，这仅通过文本实现，没有伴随通常的翻白眼动作。

AI 还在提示任务中超越了人类——基本上是理解暗示信息。源自评估精神分裂症的测试，它测量依赖于记忆和认知能力来编织和评估连贯叙述的推理能力。

参与者和 AI 模型都被给了 10 个短剧本，每个剧本描绘了日常社交互动。故事以一个暗示性的提示结束，要求参与者给出开放式的回答。在 10
个故事中，GPT-4 胜过了人类。

对研究作者来说，这些结果并不意味着大模型已经具备了心智理论。每个 AI
在某些方面都存在困难。相反，他们认为这项研究强调了使用多种心理学和神经科学测试的重要性——而不是依赖任何一种探究机器心智的模糊内在工作原理。

心理学工具可以帮助我们更好地理解大模型如何“思考”，进而帮助我们构建更安全、更准确、更值得信赖的 AI。

人工心智理论可能并不是一个遥远的概念。

  

推荐阅读

[Graph RAG：从大规模文档中发现规律，找到相互关系，速度更快，信息更全面！](http://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&mid=2247486198&idx=1&sn=fe870f73635f7e97d576fb81c20befe2&chksm=c3546865f423e173293ec3697258a848a7dff22690a4b9cad0a91abdce7745760d98c5b16281&scene=21#wechat_redirect)  

[利用AI大模型，将任何文本语料转化为知识图谱，可本地运行](http://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&mid=2247485511&idx=1&sn=48398e8c05077a9e202e729771a27452&chksm=c3546ad4f423e3c205211f27169f6861c90450df19ac47518048242f8d11e1d023cefe22c084&scene=21#wechat_redirect)  

[最具代表性的文本数据集：覆盖32个领域，444个数据集，774.5TB数据量](http://mp.weixin.qq.com/s?__biz=Mzk0OTY0NzM1Ng==&mid=2247486148&idx=1&sn=6cf9d475da4efa7521cb08f2835b8ad8&chksm=c3546857f423e141806236ba0a96fdc5e5bd16c5ca735361a9f50dbffec57fbdc4a521f7c1b4&scene=21#wechat_redirect)