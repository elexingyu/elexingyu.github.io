---
layout: post
title: "AI21Labs实践经验：让大模型从“新奇玩具”到生产力工具"
date: 2024-04-23
tags: ['AI', '教程']
style: huoshui
---

来源：A121 Labs

编译：Gavin

> AI21 Labs
> 成立于2017年，总部位于以色列特拉维夫，是一家专注于开发先进人工智能工具的公司。其利用先进的大模型和自然语言技术，为包括《财富》世界500强在内的客户提供服务，并构建了强大的客户群。

* * *

大模型（LLM）正在革新企业运营模式。

然而，尽管媒体花了一年时间大肆宣扬其在节约成本、提高效率和客户支持方面的潜力，但现实远没有那么令人兴奋：无数项目启动后很快就因严重的障碍而夭折。

本文将深入探讨阻碍企业采用大模型的复杂因素，以及探讨如何通过任务驱动的方式应对这些挑战。

## 究竟什么是大模型

AI在21世纪20年代前半叶如火如荼，但人们很容易忘记机器学习自20世纪40年代就已存在。

传统的机器学习模型本质上是算法：给定输入和预期输出，模型通过试错构建并更新从A到B的路径。机器通过更新自己的方法，找出实现目标的途径。其中最重要的是超参数——无法学习、需要用户调整的参数。

深度学习的发展极大地促进了这一过程。为了在不增加太多成本的情况下提高AI的复杂性，深度学习将语言等无限复杂的主题分解为相关的复杂层次。

设想一个金字塔——底部是字母表的基本字母，然后是这些字母组成常见单词，再然后是决定单词排列顺序的语法规则。最后的催化剂是当今AI热潮的变革模型。

为了最好地描述这一点，可以想象一个纬度和经度都被引用的地图坐标（例如，巴黎位于48.864716, 2.349014）。

大模型基本上对单词做了同样的事情——但不是2个维度，而是包括数千个维度。这样，英语中的所有单词都可以相互映射。

变革架构是处理同音异义词（即具有多个含义的单词）的方式。根据上下文，同一个单词可以用不同的向量表示。例如，有一个向量表示银行（建筑物），另一个向量表示河岸。

在海量数据集上训练后，这些AI模型背后的统计引擎现在能够接受输入数据（即最终用户的查询），并生成相关文字。因此，人们对如何在组织内应用这些模型产生了极大的兴趣。

## 为什么企业要使用大模型

鉴于用例的极大灵活性，使用潜力取决于使用它们的地方。

在客户服务领域，迅速响应至关重要，因为客户期待他们的支持问题和购买咨询能够得到即时反馈。

然而，自动化回复系统常常无法提供真正的一对一帮助，导致客户与那些无法准确识别问题并给出有效答案的聊天机器人进行无效对话，最终造成客户不满、销售机会的流失以及负面口碑的产生。

面向客户的应用只是众多应用之一；内部审视可以发现一些更大的时间节约。IBM已经意识到了这一点，为其25万名员工部署了一个AskHR应用程序。

该应用程序可以快速回答员工有关人力资源事务的问题，节省了员工搜寻晦涩文档或麻烦人力资源部门的时间。

尽管正在逐步获得关注，但我们需要正视算法中的一个关键难题：模型存在一些顽固的问题，阻碍了许多早期采用尝试。

## 企业部署大模型的难点

企业采用的担忧很多，有许多问题值得深入探讨。表面上，担忧体现在使用模型的投资回报率：毕竟，尽管可以执行许多任务，但并非所有任务都具有商业价值。然而，更深层次的担忧在于处理和生成自然语言的核心方式。

### 幻觉

传统软件依赖于明确的数据。要求计算机将2乘以3，它完全不会遇到任何问题。另一方面，自然语言充满了灰色地带。人类通过查看每个单词周围的上下文来解决这个问题——即便如此，我们也并不总是成功。

为了更深入地挖掘模型如何处理和不处理这个问题，让我们回到将单词转换为向量的概念。还记得"bank"这个词是如何根据它是建筑物还是河流特征而被赋予2个不同的代码吗？

大模型还必须处理相反的情况：识别指代同一底层事物的不同术语。这个过程称为规范化。在日常对话中，"乔·拜登"、"拜登总统"和"拜登"通常可以互换。没有确定的规则可以知道所有这些都指的是一个人——相反，它需要对外部世界的预先知识。

为了提升模型的理解深度，一些基础模型采取了增加一层语义识别的策略，将不同的名称变体统一归类到同一个标识符下。

例如，谷歌的企业知识图谱就是运用这种方法，将“乔·拜登”、“拜登”和“拜登总统”等不同称谓映射到同一个唯一标识符“/m/012gx2”上。

然而，这种做法存在一个明显的问题：它需要持续不断地更新庞大的世界实体知识库。鉴于当前公开可获取的知识库覆盖范围极广，其底层数据量也随之巨大。这不仅让部署过程变得耗时且对资源的需求量大，而且系统的稳定性也相对脆弱。

以OpenAI的GPT-3为例，该模型依赖于一个包含12,288个维度的词向量空间——换句话说，每个词条都通过一个由12,288个数值构成的向量来表征，每个维度都能够为模型捕捉词义和上下文提供更多信息。

随着模型在处理文本时逐层深入，对整段文本的理解也逐渐变得更加精准。

尽管这种技术极为先进，但错误的分类和识别仍然可能干扰模型的判断。这就是为什么有时候你会看到像Bard这样的系统错误地声称詹姆斯·韦伯太空望远镜拍摄到了太阳系外行星的首张照片。

更严重的是，这种误判只是AI错觉中相对无害的一个例子。尽管在这种情况下，谷歌的股票可能是唯一的“受害者”。在实际部署中，组织必须认识并承担风险，但是对许多组织来说，这个风险实在太高了。

### 合规风险

大模型的庞大规模可能导致其输出高度不稳定。更糟糕的是，大模型是一个"黑盒"——无法理解它们如何从提示词到输出。

一个不幸的结果是，部署环境特别容易遭受数据泄露和恶意攻击。以下是一些现实世界中的例子：

#### 训练数据泄漏

尽管像GPT这样的市场领先模型，通过对其训练数据模糊处理来避免这种情况，但DeepMind的研究人员已经证明了可提取记忆的可能性——通过向机器学习模型发起查询，研究人员能够提取出与公开文本完全匹配的整段句子。

这意味着对手可以下载大量的训练数据，进而构建他们自己的数据库。

尽管这些模型还处在版权的灰色区域，但对企业来说，更加令人担忧的是那些在敏感或机密数据上训练的大型模型所带来的风险。市场领导者如GPT-4通过“对齐”模型来应对可提取记忆的问题——也就是说，训练模型以坚持其预定义的助手角色。

不幸的是，研究人员发现这种对齐措施很容易被规避——事实上，最糟糕的例子是GPT-3.5-turbo。差不多1%的生成词汇与训练数据中的序列完全相同。

一些被提取的信息包含了个人身份信息，如电子邮件地址、传真号码和邮政编码。其中85%的个人身份信息是真实的，并且与真人相关联。

虽然这种数据盗窃在日常使用中不太可能自然发生，但它们确实暴露了一个更深层次的问题：大模型存在不可预测的脆弱性。不知道它们何时何地可能泄露内部数据，可能会导致记录中最严重的内部攻击事件。

#### 恶意攻击

训练数据泄露只是提示注入攻击的一个例子。目前，这些攻击之所以特别令人担忧，可能是因为它们与外部应用程序和数据库的集成方式，使得潜在攻击的形式更加多样化。

英国国家网络安全中心（NCSC）给出了一个例子：一家银行构建了一个智能助手来处理账户隐藏的问题。攻击者可以将传统欺骗与提示注入攻击相结合，将提示在小额的恶意交易中。

当用户询问聊天机器人“为什么我这个月的消耗更多？”时，机器人在分析交易的过程中隐藏了在付款发票中的提示，并遵循其中包含的指令。这样，机器人可能被劫持，直接将用户的信息或资金发送给攻击者。

其他例子同样令人不寒而栗：被用于检测和警告用户钓鱼邮件的电子邮件应用程序，可能被钓鱼者欺骗，在其邮件中增加一段时间不可见的文字。

可能隐藏的文本指示将邮件添加到“合法”列表中，即使它实际上是一次钓鱼尝试。由于应用程序将邮件内容的这一部分解释为指令，因此错误因此钓鱼邮件剔除为合法邮件，使模型面临进一步挑战的风险。

从训练数据中提取对话和数据集的多样性，可能导致模型在部署后难以持续保持安全。

同时这使得越来越难以克服模型面临的挑战，但我们必须牢记一个关键因素：规模。

## 为什么更小和特定模型是解决方案

在研究人员探索可扩展性提取时，他们注意到了一个有趣的现象。一些最大、资金充裕的模型——比如GPT，相较于小模型，泄露的训练数据量要多得多。

通过推断这些数据，人们计算出GPT的总可提取记忆量是其小型模型的五倍之多。这是因为GPT运行在超大规模参数上：支撑这种规模的唯一方式是对模型进行“过度训练”。

这样的结果就是大模型会死记硬背它的训练数据，并在不可预测的时刻重复它们。

在大模型开发中遇到的许多问题都源自同样的原因。有鉴于此，我们来探讨成熟的大模型实施方案是如何避免这些问题的，从而保护投资回报率（ROI）和声誉不受损害。

### 明确你的用例

大模型在许多组织中曾一度成为“新奇玩具”。然而，通用大模型的ROI始终低迷。为了解决这个问题，最成功的AI实施案例是针对特定任务进行训练和部署。在采纳这项新技术之前，成功的组织会制定策略，精确评估哪些部署场景能带来最大的益处。

引入特定任务模型（Task-Specific
Models，TSM）是解决之道。TSM是优化后的小模型，经过训练在特定的生成式AI任务中表现卓越。TSM的核心是专门为某一特定自然语言处理能力训练和设计的语言模型。

由于这些模型为特定目的“打磨”，它们的输出更加准确（不太可能包含明显错误）、可靠（产生一致类型的输出）和贴合（与提供的上下文相符）。

但TSM不仅仅是专门的语言模型，还包含了各种验证机制，围绕核心模型构建，使得它们更易于实施、更安全且性能更佳。

因此，为了从使用大模型中获得最大效果，决定哪个团队或项目需要最多的支持至关重要。接下来，将AI的需求分解为更精确的目标也很有帮助。

以下是一些应用场景：

#### 快速获取相关回答

这个用例将TSM定位为项目专家。模型专门针对某个项目的白皮书和文档进行训练，能够提供精确且切题的细节。

#### 生成摘要

当员工需要快速获取内容摘要时，大模型庞大规模可能会大幅增加产生错觉的风险。TSM通过专注手头的数据来解决这一问题，消除了过度训练的问题。

#### 语义搜索

无需逐一查阅每份文件即可快速获得相关洞见，这可以显著提高员工完成任务的速度。TSM通过在找不到相关数据时简单地结束对话来提供一种更安全的方式，而不是产生错觉或响应恶意提示。

### 构建正确的技术框架

确定用例后，成功的大模型项目接下来会构建一个坚实的框架。这需要明确的规则、流程和基准，这些都有助于定义大模型将如何嵌入到当前的运营体系中。框架必须涵盖数据管理、模型训练与精修、与现有IT系统整合以及遵守法律标准等方面。

它还应详细说明跟踪模型性能、解决偏见和伦理问题、保护数据机密性和完整性的方法。一个有序的框架为每个项目的成功打下了坚实的基础。

### 试点阶段充分收集反馈

从试点项目开始，企业可以集中评估大模型在满足特定商业需求方面的表现，以便了解使用大模型的优势和劣势。

此外，通过试点项目，企业可以识别并解决使用大模型等技术、流程或合规性难题。这种方法还有助于确保投资的安全，因为团队可以在投入更多资源到大规模计划之前看到真正的效益。

试点项目中应该包含一个一旦投入生产就能轻松扩展的框架，确保AI系统足够强大，适合大规模部署。

### 利用数据不断改进

持续迭代要求你的大模型项目有效，并根据反馈和发现调整策略。

这确保了你的计划能够适应新机遇，应对随着商业环境变化而出现的新挑战。参与迭代循环不仅提高了大模型的使用效率和效果，还使组织能够实现投资回报最大化。

尽管TSM是为特定项目明确的需求而构建的，但它们的架构允许在组织内不同领域部署多个TSM。

这意味着，一旦您开发出实施一个TSM的流程，就可以高效地应用到其他集成中去。

